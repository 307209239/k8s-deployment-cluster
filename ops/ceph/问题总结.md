## 问题总结

**问题1 每个OSD上的PG数量小于最小数目30个**
```
$ ceph -s
  cluster:
    id:     7e2de501-7b34-4121-a431-0776ee1cb004
    health: HEALTH_WARN
            too few PGs per OSD (2 < min 30)
  services:
    mon: 3 daemons, quorum k8store01,k8store02,k8store03
    mgr: k8store02(active), standbys: k8store01, k8store03
    mds: cephfs-1/1/1 up  {0=k8store03=up:active}, 2 up:standby
    osd: 18 osds: 18 up, 18 in
  data:
    pools:   2 pools, 16 pgs
    objects: 22  objects, 2.2 KiB
    usage:   18 GiB used, 33 TiB / 33 TiB avail
    pgs:     16 active+clean
```
从上面可以看到，提示说每个osd上的pg数量小于最小的数目30个。pgs为 16，因为是3副本的配置，所以当有18个osd的时候，每个osd上均分了16/18 * 3=2个pgs,也就是出现了如上的错误 小于最小配置30个。集群这种如果进行数据的存储和操作，会引发集群卡死，无法响应io，同事会导致大面积的 osd down。必须选择pg_num的值，因为它无法自动计算。  

以下是常用的一些值：
 * 少于 5 OSDs pg_num 设置为 128
 * 在 5 到 10 OSDs 之间 pg_num 设置为 512
 * 在 10 到 50 OSDs 之间 pg_num 设置为 1024

如果超过50个OSD，则需要自己计算pg_num值以及pg_num值，可使用pgcalc工具。

**问题2 设置pg_num提示错误**
```
$ ceph osd pool set cephfs_data pg_num 1024
Error E2BIG: specified pg_num 1024 is too large (creating 1016 new PGs on ~8 OSDs exceeds per-OSD max with mon_osd_max_split_count of 32)
```
以上原因是因为是一次增加的数量有限制。可以通过以下操作进行进行扩展：
```
$ ceph osd pool set cephfs_metadata pg_num 32
$ ceph osd pool set cephfs_metadata pgp_num 32

$ ceph osd pool set cephfs_metadata pg_num 64
$ ceph osd pool set cephfs_metadata pgp_num 64
```
**问题3 mgr模块无法监听地址**
```
Jan 22 10:48:21 k8store01 ceph-mgr: ChannelFailures: error('No socket could be created',)
Jan 22 10:48:21 k8store01 ceph-mgr: [22/Jan/2019:10:48:21] ENGINE Bus STOPPING
Jan 22 10:48:21 k8store01 ceph-mgr: [22/Jan/2019:10:48:21] ENGINE HTTP Server cherrypy._cpwsgi_server.CPWSGIServer(('::', 9283)) already shut down
Jan 22 10:48:21 k8store01 ceph-mgr: [22/Jan/2019:10:48:21] ENGINE No thread running for None.
Jan 22 10:48:21 k8store01 ceph-mgr: [22/Jan/2019:10:48:21] ENGINE Bus STOPPED
Jan 22 10:48:21 k8store01 ceph-mgr: [22/Jan/2019:10:48:21] ENGINE Bus EXITING
Jan 22 10:48:21 k8store01 ceph-mgr: [22/Jan/2019:10:48:21] ENGINE Bus EXITED
Jan 22 10:48:21 k8store01 ceph-mgr: 2019-01-22 10:48:21.395684 7fad9120d700 -1 log_channel(cluster) log [ERR] : Unhandled exception from module 'prometheus' while running on mgr.k8store01: error('No socket could be created',)
Jan 22 10:48:21 k8store01 ceph-mgr: 2019-01-22 10:48:21.395699 7fad9120d700 -1 prometheus.serve:
Jan 22 10:48:21 k8store01 ceph-mgr: 2019-01-22 10:48:21.395701 7fad9120d700 -1 Traceback (most recent call last):
Jan 22 10:48:21 k8store01 ceph-mgr: File "/usr/lib64/ceph/mgr/prometheus/module.py", line 718, in serve
Jan 22 10:48:21 k8store01 ceph-mgr: cherrypy.engine.start()
Jan 22 10:48:21 k8store01 ceph-mgr: File "/usr/lib/python2.7/site-packages/cherrypy/process/wspbus.py", line 250, in start
Jan 22 10:48:21 k8store01 ceph-mgr: raise e_info
Jan 22 10:48:21 k8store01 ceph-mgr: ChannelFailures: error('No socket could be created',)
```
主要是因为在centos7中将ipv6关闭了所以报错了，mgr默认是同时开启ipv4和ipv6，解决方案是指定使用ipv4启动mgr。
```
$ ceph config-key set mgr/dashboard/server_addr 0.0.0.0
$ ceph mgr module disable dashboard
$ ceph mgr module enable dashboard
```
参考链接：
[cephfs pg数量规划参考](http://docs.ceph.com/docs/mimic/rados/operations/placement-groups/)  
[cephfs pg数量计算工具](http://docs.ceph.com/docs/mimic/rados/operations/placement-groups/)  
[cephfs 元数据数量参考](https://ceph.com/planet/cephfs-ideal-pg-ratio-between-metadata-and-data-pools/)  
